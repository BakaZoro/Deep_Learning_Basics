# DeepLearning

This repository shall contain the different programs that I will have tried over time:

ğŸ. ğ¢ğ«ğ¢ğ¬ğğšğ­ğš_ğ§ğ¨ğ«ğ¦ğšğ¥.ğ©ğ²: MLP on IRIS dataset,Z-score Normalisation, 10 fold cross validation, 1 Hot encoding, 3 layer MLP with sigmoid at hidden layer and softmax at output. Weight initialisation for each fold is done 10 times. And since 10 fold cross validation in itself is repeated 100 times, that makes it 1000 times. No regulirasation is done. Finally the model is compiled, tested and trained with 1000 epochs with each weight initialisation. All the output accuracies are written in a file.

ğŸ. ğ¢ğ«ğ¢ğ¬ğğšğ­ğš_ğ¥ğŸ.ğ©ğ²: MLP on IRIS dataset,Z-score Normalisation, 10 fold cross validation, 1 Hot encoding, 3 layer MLP with sigmoid at hidden layer and softmax at output. Weight initialisation for each fold is done 10 times. And since 10 fold cross validation in itself is repeated 100 times, that makes it 1000 times. ğ‹ğŸ ğ«ğğ ğ®ğ¥ğ¢ğ«ğšğ¬ğšğ­ğ¢ğ¨ğ§ using a ğ©ğğ§ğšğ¥ğ­ğ²(ğ¥ğšğ¦ğğš ğ¯ğšğ¥ğ®ğ) ğ¨ğŸ ğŸ.ğŸğŸ is done. Finally the model is compiled, tested and trained with 1000 epochs with each weight initialisation. All the output accuracies are written in a file.

ğŸ‘. ğ¢ğ«ğ¢ğ¬ğğšğ­ğš_ğ¥ğŸ.ğ©ğ²: MLP on IRIS dataset,Z-score Normalisation, 10 fold cross validation, 1 Hot encoding, 3 layer MLP with sigmoid at hidden layer and softmax at output. Weight initialisation for each fold is done 10 times. And since 10 fold cross validation in itself is repeated 100 times, that makes it 1000 times. ğ‹ğŸ ğ«ğğ ğ®ğ¥ğ¢ğ«ğšğ¬ğšğ­ğ¢ğ¨ğ§ using a ğ©ğğ§ğšğ¥ğ­ğ²(ğ¥ğšğ¦ğğš ğ¯ğšğ¥ğ®ğ) ğ¨ğŸ ğŸ.ğŸğŸ is done. Finally the model is compiled, tested and trained with 1000 epochs with each weight initialisation. All the output accuracies are written in a file.

ğŸ’. ğŸ‘ğğŸ.ğ©ğ²: A simple python 3 program for plotting points in 3D, which shall be used in later projects too.

ğŸ“. ğ’ğ¡ğ®ğ­ğ­ğ¥ğ.ğ©ğ²: The ğˆğ‘ğˆğ’ ğğšğ­ğšğ¬ğğ­ contains 150 samples having 3 classes. Each class has 50 samples each. So when the model is trained with "ğ¬ğ¢ğ ğ¦ğ¨ğ¢ğ" as the output function, the initial predictions may/may not be that good. However, what is definitely true that with each epoch, the predicted values move nearer to the actual value. And since the output function used is "ğ¬ğ¢ğ ğ¦ğ¨ğ¢ğ", it will never give exactly 1 or 0. So the program ğ’ğ¡ğ®ğ­ğ­ğ¥ğ.ğ©ğ², trains a 3 layered MLP on the IRIS dataset and accesses the predicted values for each sample during each epoch. Each predicted value is joined to the actual value by a 'ğ ğ«ğğğ§ ğ¥ğ¢ğ§ğ'. So each cluster of values move nearer to the actual value, causing the line between them to shorten thus making it look like a shuttle moving closer. Each graph is saved as an image and then all those images sequences are then used to compile a video, which is basically a video of how the model learns to predict.

ğŸ”. ğ•ğ¢ğğğ¨.ğ©ğ²: A python program that finds a set of images and then formas a video using those image seqeunces.

ğŸ•. ğŒğğˆğ’ğ“_ğŒğ‹ğ.ğ©ğ²:MLP on MNIST dataset,Z-score Normalisation, 10 fold cross validation, 1 Hot encoding, 3 layer MLP with sigmoid at hidden layer and softmax at output. Weight initialisation for each fold is done 10 times. And since 10 fold cross validation in itself is repeated 100 times, that makes it 1000 times. No regulirasation is done. Finally the model is compiled, tested and trained with 20 epochs with each weight initialisation. All the output accuracies are written in a file.

ğŸ–. ğ•ğ¨ğœğšğ›ğğŸğ–ğ¨ğ«ğğ¬.ğ¢ğ©ğ²ğ§ğ›: When dealing with texts as features, the texts need to be pre-processed. One such method is the Bag Of Words Method or BOW. In this the main important words are added to a dictionary and then each sentence is converted to a vector depending on the presence of a particular word in the dictionary. If the word is present then it is 1 else 0. This comes under the feature extraction part. The following code does so. This one creates a dictionary of words.

ğŸ—. ğ“ğğ±ğ­ğŸğğ®ğ¦.ğ¢ğ©ğ²ğ§ğ›: When handling text data for Machine Learning problems, the data needs to be pre-processed. Even though quite a few ML algorithms nowadays can handle textual data without being converted into numerical data nut many that are implemented using the sklearn library cannot. The tree methods present require the textual data to be converted into numerical form. Hence Text2Num contains three such methods, "CountVectorizer, "TF-IDF Vectroizer" and "Hashing Vectorizer" that help in converting text data to machine readable format.

 
